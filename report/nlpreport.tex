%
% File acl2014.tex
%
% Contact: koller@ling.uni-potsdam.de, yusuke@nii.ac.jp
%%
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
  \usepackage{acl2014}
  \usepackage[utf8]{inputenc}
  \usepackage{times}
  \usepackage{url}
  \usepackage{latexsym}
  
  %\setlength\titlebox{5cm}
  
  % You can expand the titlebox if you need extra space
  % to show all the authors. Please do not make the titlebox
  % smaller than 5cm (the original size); we will check this
  % in the camera-ready version and ask you to change it back.
  
  
  \title{Emotion Classification on Twitter Data.
  Multi-Label Classification
  }
  
  \author{
  Yousaf Khan\\
  IMS\\
  University of Stuttgart\\
  (\texttt{yousaf.khan}
  \And
  Aysoltan Gravina\\
  IMS\\
  University of Stuttgart\\
    \texttt{aysoltan.gravina}\\
    \texttt{@ims.uni-stuttgart.de}
    \And
  Dhruv Mishra\\
  IMS\\
  University of Stuttgart\\
    \texttt{dhruv.mishra})
  }
  \date{\today}
  
  \begin{document}
  \maketitle
  \begin{abstract}
  There are more than 200 million Twitter accounts, 180 thousand tweets posted every day, and 18 thousand Twitter search queries every second. People use Twitter primarily to converse with other individuals, groups, and the world in general (Boyd et al., 2010). Emotions detection in social media posts is growing in importance for industry, health, and security.
  In this paper, we describe how we have created a Multi-label classification system for tweets scraped from Twitter using different emotion classes. We conduct experiments to classify the tweet according to eight different emotions of happy, sad, angry, disgust, surprise, fear, trust, and love. We used two classifiers Naïve Bayes and Perceptron on our data set with different features to train our model and evaluate the results based on statistical measures.
  
  
  \end{abstract}
  
  
  
  \section{Introduction}
  
  Emotions expressed in microblogs have a number of applications ranging from determining the popularity of products and governments (Mohammad and Yang, 2011) to improve human-computer interaction (Velasquez, 1997; Ravaja et al., 2006).
    Twitter is a popular online microblogging service where people post and express themselves in the form of tweets/retweets, which are up to 140 characters long. Some words in a tweet are preceded with a hash symbol (\#) and are called hashtags. Hashtags indicate mostly the topic of a tweet and can also express the tone of the message or their internal emotions. It also plays a role in search and allows searching through the hash tagged words.
    The tweets were generated by millions of different individuals (with very different educational and socioeconomic backgrounds) and are accessible to all. The conversations can take on non-traditional forms. The goal of the development of the models for emotion analysis on Twitter data is to understand the psychology of conversations, prevailing trends, opinions and the driving factors for a particular emotion.
  
  We use labeled data scraped from Twitter as our training data. Each row comprises a set of attributes like emotion label, tweet id, user, date and time, language, tweet, Retweet, and URL.
       
  We have used supervised methods and n-gram features such as unigrams and bigrams (individual lower cased words and two-word sequences) for emotion detection (Aman and Szpakowicz, 2007; Neviarouskaya et al., 2009; Mohammad, 2012b).
  
  In this paper, we show how we trained our model for supervised classifiers of Perceptron and Naïve Bayes. We measure the performance of both the classifiers on statistical measures of accuracy, precision, recall, and F-score and tried to determine the best classifier.
   
  We compiled labeled data for eight emotions - anger, disgust, fear, happy, love, sad, surprise, trust - argued to be the most basic (Ekman, 1992) and show through experiments that the emotions are not always consistent, but match the intuitions of trained judges.
  
  \section{Method}
  
  \subsection{Data Cleaning and Preprocessing}
  
  We were given crawled data set of twitter from different topics, emotions, demographics and time. Each twitter record in training data set more or less followed the structure consisting of labeled emotion, date and time, tweet id, username, language, tweet, retweet, and URL. The data was inconsistent in terms of certain fields. The labeled data set is preprocessed to remove junk information and to obtain a better-structured text. Following are the main steps applied as part of the pre-processing steps:
  
  \begin{itemize}
  \item Tweet containing RT tag was a retweet (response) to the user who generated the tweet. We removed all these retweets to avoid biasness in results causing from repetitions of original tweet
  \item Tweets containing @ symbol indicated the user of the tweet. We removed all the tags having @ as the user name does not play a role in correct classification.
  \item All the hashtags indicating emotion label in a tweet were removed from dataset to avoid biasness in classification.
  \item Url from each record was removed for training phase better prediction of labels on test data in classification.
  \item Tweet Ids were removed.
  \item For training phase, we focused mainly on tweets in English. So we filtered out English tweets and removed the language column.
  \item We ignored date and time field for our training dataset.
  \item   Newline present in a tweet is replaced by empty string.
  \item   Only tweet which was not empty after pre-processing steps was used for training.
  \end{itemize}
  
  
  \subsection{Perceptron}
  
  A Perceptron learning algorithm learns the weights for different features which account for the dimensions on a hyperplane and helps in dividing the feature space. We implemented a multi-class perceptron with an idea to find one winning perceptron per class.
    During training, we started with random initialization of weights with a bias of 0.1 for each feature. We used a sparse representation so only those features which had non zero values were stored for a given tweet. The training process involved 100 epochs, where the weights improve with each epoch. During each iteration (epoch) we get a winning perceptron with highest value of argmax-y, where we compute dot product for each class and chose one with the highest score, with
    {$$arg max\textsubscript{i} = \sum\limits {i=1}^n f(x)\textsubscript{i} * w\textsubscript{i}$$}
  
  Where n is the number of features, f(x) is the feature vector and w is the weight vector. Considering the winning perceptron as predicted label for the instance in the iteration. We further adjusted the weights as explained in Algorithm 1.
  
  \noindent\hrulefill\\
   \textbf{\emph {Multi-Class Perceptron and Weights Adjusting}} \\
  
  \emph{ If predicted class is correct, no change}
  
  \emph{If predicted class is wrong, i.e. different from gold:}
  
  \emph{- Lower the score of the wrong answer:}
  
  \emph{w\textsubscript{i} = w\textsubscript{i} - f(x)\textsubscript{i}}
  
  \emph{Raise the score of right answer:}
  
  \emph{w\textsubscript{i} = w\textsubscript{i gold} + f(x)\textsubscript{i} }
  
  \noindent\hrulefill\\
  \textbf{\emph{ Algorthim 1: Perceptron }}
  
  In this algorithm, w\textsubscript{i} is the weight vector for perceptron of predicted class, that we will minimize if the predicted label is different from the gold label. Similarly w\textsubscript{i gold} is the weight vector for perceptron of the gold label, that we will increase in case predicted and gold label gives a match.
  In the testing phase, we used the trained weights to predict arg max value. For each tweet instance in the test set, we extracted the features and computed the
  winning perceptron on the basis of given trained weight vectors in the same way as during the training. The highest “argmax” value is referred as the
  predicted label.\\
  
  \subsection{Naive Bayes}
  
  
  
  \subsection{Features}
  
  We implemented different features for multi-label classification which includes features like:
  
  
  {\bf Stems}
  
  
  Token based stems were extracted with Stanford and Snowball parsers.We used both the parsers to observe variations in the result set. We tokenized the vocabulary into respective stems using both Stanford and Snowball parsers.
  ...\\
  
  
  {\bf Ngrams}
  
  Firstly the tweets were tokenized with Ark-TweetsTokenizer.Unigrams and bigrams were extracted from tweets to be considered as features. Both unigrams and bigrams are considered as a single feature to look for impact on results for perceptron and Naive Bayes.
  
  
  {\bf Word-Classes}
  
  
  The intention is to determine which word class express more emotion and whether one of the word-classes contribute to emotion classification\\
  
  
  {\bf Negation feature}
  
  
  We used negation feature to identify the tweets which contain negated phrases like “not really happy”. Such phrases can result in wrong
  classification if only unigrams and bigrams are considered. We add as negation feature, the three unigram following the negation word. We used
  following negation words:
  
  "aren’t", "arent", "cannot", "cant", "couldnt", "couldn’t", "didn’t", "didnt", "doesnt", "doesn’t", "dont", "don’t", "hadn’t", "hadnt", "hasn’t",
  "hasnt", "havent", "haven’t", "isn’t", "isnt", "neither", "never", "no", "nobody", "none", "nor", "not", "nt", "n’t", "shouldnt", "wasnt", "wasnt"\\
  
  {\bf Parts of speech}
  
  
  We used feature combination of Unigrams with the part of speech that they belong to. Our intention for considering part of speech as a feature is to discover
  whether there could be a possibility in better classification results of emotions. Nouns, adjectives, and verbs are considered in combination
  with unigrams in training model. \\
  
  {\bf NRC Lexicon}
  
  The NRC Emotion Lexicon comprises of a list of English words and their associations with eight basic emotions and two sentiments (negative and
  positive). Currently, it contains 14,182 unigrams for English along with other languages. The annotations were manually done by means of crowd sourcing. For example, below listed are the entries from the NRC dictionary for the word excitement.\\
  
  \begin{table}[h]
  \begin{center}
  \begin{tabular}{|l|r|}
  \hline \bf Emotion & \bf Value\\ \hline
  Anger & 0\\
  Disgust & 0\\
  Happy & 1\\
  Sad & 0\\
  Love & 1\\
  Surprise & 1\\
  Fear & 0\\
  Trust & 0\\
  Positive & 1\\
  Negative & 0\\
  \hline
  \end{tabular}
  \end{center}
  \caption{\label{font-table} NRC Emotion lexicon for "Excitement"}
  \end{table}
  
  According to the values of the word for each emotion in NRC lexicon, we added a feature in our model. For instance in Table, for word \emph{excitement} in tweet we
  we added features for happy, surprise etc and in our model.
  
  \section{Experiment}
  
  \subsection{Feature Selection}
  
  In order to select the best features, we tested each feature separately and evaluated using micro and macro F-Score and then combined the features
  with better result to get higher F-Score. Table 2 shows the result of this feature selection experiment.
  
  \subsection{Feature Evaluation}
  
  From the table, it can be seen that the micro and macro F-Score is lower/higher/no considerable improvement (yet to be inserted)
  
  
  In the case of ( ) feature, even though it did not contribute much in improving the micro and macro F-scores of all classes.
  
  
  The F-score of the class disgust was increased significantly which was otherwise very poor. This class was at least presented in the training set
  
  \subsection{Parameter Selection}
  
  
  We tried also different feature parameters:
  
  
  normalized Frequencies, term frequency inverse document frequency (TF-IDF), binary. The binary Parameters gave better results as compared to other
  parameters.
  
  
  \subsection{Perceptron Settings}
  We tried different repetition settings for perceptron. We trained our model for different Epochs settings:
  50, 100, 150, 200 and 250. The best result was with 200 epochs across the dataset.
  
  We adjusted the bias for each epoch. Initially, bias was set 0 then gradually altered the figure to 0.1, 0.5 and 1.0 for training the model.
  
  \subsection{Result}
  
  Best performance for the system was attained when the combination of features Ngram + ... were used. We used statistical measures of accuracy,
  precision, recall, and F-score for emotions in our model. We are interested in showing micro and macro F-score for each individual emotion with
  various feature combinations.
  Typical features that we considered in corresponding results for all eight
  emotions and their micro and macro F-score are given in the table.
  
  However poor performance was observed for some classes compared to other, which is why we conducted an error analysis study on the obtained result.
  
  \section{Future Work}
  
  Future work in multi label classification of emotions from twitter can be extended to all the languages. For now, our work focus is mainly on
  English tweets as it give us flexibility in making our model understand about emotions in tweets. Training the model with a significant interpretation
  of emoticons can be useful in determining emotion from tweets. The ambiguity in use of emoticons makes it hard for better training of the model.
  
  
  All the stop words including determiners, punctuation marks, and functional verbs have to be removed according to criteria that maximize the learning efficiency of model. For instance full stops, comma or functional verbs like “to be, have,
  is” needs to be removed from training set as these are frequently occurring words and does not show any clue in emotion detection.
  
  
  Words with longer length are often less frequent in the corpus and give very low indication in emotion detection. Words segmentation of such longer
  words need to be formulated in a better way to maximize the learning efficiency of the model and ultimately better performance on the test set.
  
  
  
  -------------------------------------------------------------
  
  \begin{table*}[t]
  \small
    \centering
  \begin{tabular}{|l|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}}
  \hline
  \begin{tabular}[c]{@{}l@{}}\end{tabular} & \bf Ngrams & \bf Ngrams+ Negation & \bf Ngram+NRC & \bf Ngram+ Negation+ NRC
  \\ \hline
  \bf Happy & 0.78 & 0.8 & 0.78 & 0.70 \\ \hline
  \bf Sad & 0.78 & 0.69 & 0.70 & 0.64 \\ \hline
  \bf Anger & 0.75 & 0.63 & 0.66 & 0.66 \\ \hline
  \bf Surprise & 0.75 & 0.67 & 0.52 & 0.57 \\ \hline
  \bf Disgust & 0.65 & 0.59 & 0.46 & 0.42 \\ \hline
  \bf Fear & 0.74 & 0.64 & 0.65 & 0.66 \\ \hline
  \bf Love & 0.61 & 0.57 & 0.46 & 0.56 \\ \hline
  \bf Trust & 0.45 & 0.37 & 0.27 & 0.24 \\ \hline
  \bf Macro F-score & 0.69 & 0.62 & 0.56 & 0.56 \\ \hline
  \end{tabular}
    \caption{ F-Score per category using Snowball Stemmer(training set) }
    \label{tab:1}
  \end{table*}
  -----------------
  
  \begin{table*}[t]
  \small
    \centering
  \begin{tabular}{|l|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}}
  \hline
  \begin{tabular}[c]{@{}l@{}}\end{tabular} & \bf Ngrams & \bf Ngrams+ Negation & \bf Ngram+NRC & \bf Ngram+ Negation+ NRC
  \\ \hline
  \bf Happy & 0.67 & 0.72 & 0.77 & 0.41 \\ \hline
  \bf Sad & 0.63 & 0.70 & 0.72 & 0.18 \\ \hline
  \bf Anger & 0.63 & 0.67 & 0.70 & 0.03 \\ \hline
  \bf Surprise & 0.38 & 0.42 & 0.49 & 0.01 \\ \hline
  \bf Disgust & 0.25 & 0.32 & 0.45 & 0.0 \\ \hline
  \bf Fear & 0.57 & 0.63 & 0.66 & 0.02 \\ \hline
  \bf Love & 0.44 & 0.48 & 0.46 & 0.11 \\ \hline
  \bf Trust & 0.14 & 0.14 & 0.20 & 0.0 \\ \hline
  \bf Macro F-score & 0.46 & 0.51 & 0.56 & 0.1 \\ \hline
  \end{tabular}
    \caption{ F-Score per category using Snowball Stemmer(test set)}
    \label{tab:1}
  \end{table*}
  -------------------------------------------------------------
  
  
  \begin{table*}[t]
  \small
    \centering
  \begin{tabular}{|l|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}}
  \hline
  \begin{tabular}[c]{@{}l@{}}\end{tabular} & \bf Ngrams & \bf Ngrams+ Negation & \bf Ngram+NRC & \bf Ngram+ Negation+ NRC
  \\ \hline
  \bf Happy & 0.79 & 0.77 & 0.59 & 0.64 \\ \hline
  \bf Sad & 0.77 & 0.62 & 0.69 & 0.69 \\ \hline
  \bf Anger & 0.69 & 0.72 & 0.63 & 0.61\\ \hline
  \bf Surprise & 0.75 & 0.62 & 0.60 & 0.53 \\ \hline
  \bf Disgust & 0.60 & 0.53 & 0.51 & 0.44 \\ \hline
  \bf Fear & 0.77 & 0.60 & 0.65 & 0.60 \\ \hline
  \bf Love & 0.61 & 0.56 & 0.51 & 0.53 \\ \hline
  \bf Trust & 0.49 & 0.30 & 0.21 &0.21 \\ \hline
  \bf Macro F-score & 0.69 & 0.59 & 0.53 & 0.53 \\ \hline
  \end{tabular}
    \caption{ F-Score per category using Standford Stemmer (training set) }
    \label{tab:1}
  \end{table*}
  =======================================================
  \begin{table*}[t]
  \small
    \centering
  \begin{tabular}{|l|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}}
  \hline
  \begin{tabular}[c]{@{}l@{}}\end{tabular} & \bf Ngrams & \bf Ngrams+ Negation & \bf Ngram+NRC & \bf Ngram+ Negation+ NRC
  \\ \hline
  \bf Happy & 0.45 & 0.45 & 0.48 & 0.41 \\ \hline
  \bf Sad & 0.15 & 0.15 & 0.15 & 0.18 \\ \hline
  \bf Anger & 0.07 & 0.07 & 0.08 & 0.03\\ \hline
  \bf Surprise & 0.03 & 0.03 & 0.03 & 0.01 \\ \hline
  \bf Disgust & 0.01 & 0.01 & 0.0 & 0.0 \\ \hline
  \bf Fear & 0.07 & 0.07 & 0.07 & 0.02 \\ \hline
  \bf Love & 0.19 & 0.18 & 0.15 & 0.11 \\ \hline
  \bf Trust & 0.0 & 0.0 & 0.0 &0.0\\ \hline
  \bf Macro F-score & 0.12 & 0.12 & 0.12 & 0.1\\ \hline
  \end{tabular}
    \caption{ F-Score per category using Standford Stemmer (test set) }
    \label{tab:1}
  \end{table*}
  ========================================================
  \begin{table*}[t]
  \small
    \centering
  \begin{tabular}{|l|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}}
  \hline
  \begin{tabular}[c]{@{}l@{}}\end{tabular} & \bf Ngrams & \bf Ngrams+ Negation & \bf Ngram+NRC & \bf Ngram+ Negation+ NRC
  \\ \hline
  \bf Happy & & & & \\ \hline
  \bf Sad & & & & \\ \hline
  \bf Anger & & & & \\ \hline
  \bf Surprise & & & & \\ \hline
  \bf Disgust & & & & \\ \hline
  \bf Fear & & & & \\ \hline
  \bf Love & & & & \\ \hline
  \bf Trust & & & 1 & 1\\ \hline
  \bf Macro F-score & & & & \\ \hline
  \end{tabular}
    \caption{ F-Score with Naive Bayes per category (training set) }
    \label{tab:1}
  \end{table*}
  ------------------------------------------------------------
  
  
  \begin{table*}[t]
  \small
    \centering
  \begin{tabular}{|l|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}}
  \hline
  \begin{tabular}[c]{@{}l@{}}\end{tabular} & \bf Ngrams & \bf Ngrams+ Negation & \bf Ngram+NRC & \bf Ngram+ Negation+ NRC
  \\ \hline
  \bf Happy & & & & \\ \hline
  \bf Sad & & & & \\ \hline
  \bf Anger & & & & \\ \hline
  \bf Surprise & & & & \\ \hline
  \bf Disgust & & & & \\ \hline
  \bf Fear & & & & \\ \hline
  \bf Love & & & & \\ \hline
  \bf Trust & & & 1 & 1\\ \hline
  \bf Macro F-score & & & & \\ \hline
  \end{tabular}
    \caption{ F-Score with Naive Bayes per category (test set) }
    \label{tab:1}
  \end{table*}
  ------------------------------------------
  
  \begin{table*}[t]
  \small
    \centering
  \begin{tabular}{|l|p{1 cm}|p{1 cm}|p{1 cm}|p{1 cm}|p{1 cm}|p{1 cm}|p{1 cm}|p{1 cm}|p{1 cm}|p{1 cm}}
  \hline
  \begin{tabular}[c]{@{}l@{}}\end{tabular} & \bf Happy & \bf Sad & \bf Angry & \bf Surprise & \bf Fear & \bf Disgust & \bf Love & \bf Trust & \begin{tabular}[c]{@{}l@{}}\bf Micro\\ \bf F-Score\end{tabular} & \begin{tabular}[c]{@{}l@{}} \bf Macro\\ \bf F-Score\end{tabular} \\ \hline
  \bf 1 & 0.49 & 0.63 & 0.81 & 0.36 & 0.61 & 0.32 & 0.71 & 0.54 & 0.9 & 0.4\\ \hline
  \bf 2 & 0.78 & 0.84 & 0.88 & 0.81 & 0.81 & 0.87 & 0.84 & 0.83 & 0.9 & 0.4\\ \hline
  \bf 3 & 0.88 & 0.92 & 0.93 & 0.9 & 0.9 & 0.94 & 0.91 & 0.91 & 0.9 & 0.4\\ \hline
  \bf 4 & 0.94 & 0.96 & 0.96 & 0.95 & 0.95 & 0.96 & 0.96 & 0.96 & 0.9 & 0.4\\ \hline
  \bf 5 & 0.97 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 & 0.9 & 0.4 \\ \hline
  \bf 6 & & & 1 & 1 & 1 & 1 & 1 & 1 & 0.9 & 0.4\\ \hline
  \end{tabular}
    \caption{ Score per category }
    \label{tab:1}
  \end{table*}
  
  
  
  
  
  
  
  % include your own bib file like this:
  %\bibliographystyle{acl}
  %\bibliography{acl2014}
  
  \begin{thebibliography}{}
  
  \bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
  Alfred~V. Aho and Jeffrey~D. Ullman.
  \newblock 1972.
  \newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
  \newblock Prentice-{Hall}, Englewood Cliffs, NJ.
  
  \bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
  {American Psychological Association}.
  \newblock 1983.
  \newblock {\em Publications Manual}.
  \newblock American Psychological Association, Washington, DC.
  
  \bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
  {Association for Computing Machinery}.
  \newblock 1983.
  \newblock {\em Computing Reviews}, 24(11):503--512.
  
  \bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
  Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
  \newblock 1981.
  \newblock Alternation.
  \newblock {\em Journal of the Association for Computing Machinery},
  28(1):114--133.
  
  \bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
  Dan Gusfield.
  \newblock 1997.
  \newblock {\em Algorithms on Strings, Trees and Sequences}.
  \newblock Cambridge University Press, Cambridge, UK.
  
  \end{thebibliography}
  
  \end{document}
  