%
% File acl2014.tex
%
% Contact: koller@ling.uni-potsdam.de, yusuke@nii.ac.jp
%%
%% Based on the style files for ACL-2013, which were, in turn,
%% Based on the style files for ACL-2012, which were, in turn,
%% based on the style files for ACL-2011, which were, in turn, 
%% based on the style files for ACL-2010, which were, in turn, 
%% based on the style files for ACL-IJCNLP-2009, which were, in turn,
%% based on the style files for EACL-2009 and IJCNLP-2008...

%% Based on the style files for EACL 2006 by 
%%e.agirre@ehu.es or Sergi.Balari@uab.es
%% and that of ACL 08 by Joakim Nivre and Noah Smith

\documentclass[11pt]{article}
\usepackage{acl2014}
\usepackage[utf8]{inputenc}
\usepackage{times}
\usepackage{url}
\usepackage{latexsym}

%\setlength\titlebox{5cm}

% You can expand the titlebox if you need extra space
% to show all the authors. Please do not make the titlebox
% smaller than 5cm (the original size); we will check this
% in the camera-ready version and ask you to change it back.


\title{Emotion Classification on Twitter Data.
Multi-Label Classification
}

\author{
  Yousaf Khan\\
  IMS\\
  University of Stuttgart\\
  (\texttt{yousaf.khan}
\And
  Aysoltan Gravina\\
  IMS\\
  University of Stuttgart\\
  \texttt{aysoltan.gravina}\\
  \texttt{@ims.uni-stuttgart.de}
  \And
  Dhruv Mishra\\
  IMS\\
  University of Stuttgart\\
  \texttt{dhruv.mishra})
}
\date{\today}

\begin{document}
\maketitle
\begin{abstract}
  There are more than 200 million Twitter accounts, 180 thousand tweets posted every day, and 18 thousand Twitter search queries every second. People use primary Twitter to converse with other individuals, groups, and the world in general (Boyd et al., 2010). Emotions detecting in social media posts grows an importance for industry, health, and security.
In this paper, we describe how we created Multi-label classification system for the corpus crawled from Twitter using different emotion classes as hashtag. We conduct experiments to classify the tweet according to eight different emotions of happy, sad, angry, disgust, surprise, fear, trust and love. We used two classifiers Naïve Bayes and Perceptron on twitter data set with different features to train our model and evaluated the results based on statistical measures.


\end{abstract}



\section{Introduction}

Emotions expressed in microblogs has a number of applications ranging from determining popularity of products and governments (Mohammad and Yang, 2011) to improving human-computer interaction (Velasquez, 1997; Ravaja et al., 2006).
	Twitter is an online microblogging service where people post and express themselves in the form of tweets/retweets, which are up to 140 characters long. The words in a tweet are preceded with a hash symbol (\#) and called hashtags. Hashtags indicate mostly the topic of tweet and can also express the tone of the message or their internal emotions. It also plays a role in search and allows searching through the hash tagged words.
	The tweets were generated by millions of different individuals (with very different educational and socio-economic backgrounds) and are accessible to all. The conversations can take on non-traditional forms. The goal of the development of the models for emotion analysis on Twitter data is to understand the psychology of conversations, prevailing trends, opinions and the driving factors for particular emotion.

     We used labeled data crawled from Twitter as training data. Each row of training data comprised of set of features like emotion label, tweet id, user, date and time, language, tweet, Retweet and URL.
     
     We used supervised methods and ngram features such as unigrams and bigrams (individual lower cased words and two-word sequences) for emotion detection (Aman and Szpakowicz, 2007; Neviarouskaya et al., 2009; Mohammad, 2012b).

In this paper, we showed how we trained our model on given dataset of twitter for supervised classifiers of Perceptron and Naïve Bayes. We measured the performance of both the classifiers on statistical measures of accuracy, precision, recall and F-score and deducted the best classifier based for given training data.
 
We compiled labeled data for eight emotions - anger, disgust, fear, happy, love, sad, surprise, trust - argued to be the most basic (Ekman, 1992) and showed through experiments that the emotions are not always consistent, but match the intuitions of trained judges.



\section{Method}

\subsection{Data Cleaning and Preprocessing}

We were given crawled dataset of twitter from different topics, emotions, demographics and time. Each twitter record in training dataset more or less followed the structure consisting of labeled emotion, date and time, tweet id, username, language, tweet, retweet and URL. The data was inconsistent in terms of certain fields. The labeled data set is preprocessed to remove junk information and to obtain a better structured text. Following are the main steps applied as part of the pre-processing steps:

\begin{itemize}
\item Tweet containing RT tag was the retweet (response) to the user who generated tweet. We removed all these retweets to avoid biasness in results causing from repetitions of original tweet
\item Tweets containing @ symbol indicated the user of the tweet. We removed all the tags having @ as the user name does not play a role in correct classification.
\item All the hashtags indicating emotion label in tweet, were removed from dataset to avoid biasedness in classification
\item	Url from each record was removed for training phase better prediction of labels on test data in classification. 
\item	Tweet Ids were removed
\item	For training phase, we focused mainly on tweets in English. So we filtered out English tweets and removed the language column to be used in training
\item	We ignored date and time field for our training dataset.
\item 	Newline present in a tweet is replaced by empty string.
\item 	Only tweet which was not empty after pre-processing steps was preceded.
\end{itemize}


\subsection{Perceptron}

In this work, we use a supervised learning methods perceptron for multi-class classification. A Perceptron learning algorithm learn the weights for different features which accounts for the dimensions on hyperplane and helps in dividing the feature space. We implemented a multi-class perceptron with an idea to find one winning perceptron per class.
	For each class label on the basis of the weights will be decided if the tweet belongs to that class or not. During training, we started with random initialization of weights with bias and adjusted it with getting of the features in order to handle the sparse data. The training process involves 200 epochs, where the weights improve with each epoch. During each iteration (epoch) we get a winning perceptron with highest value argmax-y, where we compute dot product for each class and chose one with the highest score, with
	{$$arg max\textsubscript{i} = \sum\limits {i=1}^n f(x)\textsubscript{i} * w\textsubscript{i}$$}

Where n is the number of features, f(x) is the feature vector and w is the weight vector. Considering the winning perceptron as predicted label for the instance in the iteration. We further adjusted the weights as explained in Algorithm 1.

\noindent\hrulefill\\
 \textbf{\emph {Multi-Class Perceptron and Weights Adjusting}} \\

\emph{ If predicted class is correct, no change}

\emph{If predicted class is wrong, i.e. different from gold:}

\emph{- Lower the score of wrong answer:}

\emph{w\textsubscript{i} = w\textsubscript{i} - f(x)\textsubscript{i}}

\emph{Raise the score of right answer:}

\emph{w\textsubscript{i} = w\textsubscript{i gold} + f(x)\textsubscript{i} }

\noindent\hrulefill\\
\textbf{\emph{ Algorthim 1: Perceptron }}

In this algorithm, w\textsubscript{i} is the weight vector for perceptron of predicted class, that we will minimize if the predicted label is different from gold label. Similary w\textsubscript{i gold} is the weight vector for perceptron of gold label, that we will increase in case predicted and gold label gives a match.
In testing phase, we used the trained weights to predict arg max value. For each tweet instance in the test set we extracted the features and computed the
winning perceptron on the basis of given trained weight vectors in the same way as during the training. The highest “argmax” value is referred as the
predicted label.\\

\subsection{Naive Bayes}



\subsection{Features}

We implemented different features for multi-label classification which includes features like:


{\bf Stems}


Token based stems were extracted with Stanford and Snowball parsers.We used both the parsers to obsere variations in the result set. We tokenized the vocabulary into respective stems using both Stanford and Snowball parsers.
...\\


{\bf Ngrams}

Firstly the tweets were tokenized with Ark-TweetsTokenizer.Unigrams and bigrams were extracted from tweets to be considered as features. Both unigrams and bigrams are considered single feature to look for impact on results for percpetorn and naive bayes.


{\bf Word-Classes}


The intention is to determine which word-class express more emotion and whether the one of the word-classes contribute to emotion classification\\


{\bf Negation feature}


We used negation feature to identify the tweets which contain negated phrases like “not really happy”. Such phrases can result in wrong
classification if only unigrams and bigrams are considered. We add as negation feature, the three unigram following the negation word. We used
following negation words:

"aren’t", "arent", "cannot", "cant", "couldnt", "couldn’t", "didn’t", "didnt", "doesnt", "doesn’t", "dont", "don’t", "hadn’t", "hadnt", "hasn’t",
"hasnt", "havent", "haven’t", "isn’t", "isnt", "neither", "never", "no", "nobody", "none", "nor", "not", "nt", "n’t", "shouldnt", "wasnt", "wasnt"\\

{\bf Parts of speech}


We used feature combination of Unigrams with the part of speech that they belong to. Our intention for considering part of speech as feature is to discover
whether there could be a possibility in better classification results of emotions. Nouns, adjectives and verbs are considered in combination
with unigrams in training model. \\

{\bf NRC Lexicon}

The NRC Emotion Lexicon comprises of a list of English words and their associations with eight basic emotions and two sentiments (negative and
positive).Currently it contains 14,182 unigrams for English along with other languages.  The annotations were manually done by means of crowd sourcing. For example, below listed are the entries from the NRC dictionary for the word excitement.\\

\begin{table}[h]
\begin{center}
\begin{tabular}{|l|r|}
\hline \bf Emotion & \bf Value\\ \hline
Anger & 0\\
Disgust & 0\\
Happy & 1\\
Sad & 0\\
Love & 1\\
Surprise & 1\\
Fear & 0\\
Trust & 0\\
Positive & 1\\
Negative & 0\\
\hline
\end{tabular}
\end{center}
\caption{\label{font-table} NRC Emotion lexicon for "Excitement"}
\end{table}

According to the values of word for each emotion in NRC lexicon, we added feature in our model. For instance in Table, for word \emph{excitement} in tweet we
added feature of happy, surprise and in our model.

\section{Experiment}

\subsection{Feature Selection}

In order to select the best features, we tested each feature separately and evaluated using micro and macro F-Score and then combined the features
with better result to get higher F-Score. Table 2 shows the result of this feature selection experiment.

\subsection{Feature Evaluation}

From the table it can be seen that the micro and macro F-Score is lower/higher/no considerable improvement (yet to be inserted)


In the case of ( ) feature, even though it did not contribute much in improving the micro and macro F-scores of all classes.


The F-score of the class disgust was increased significantly which was otherwise very poor. This class was at least presented in the training set

\subsection{Parameter Selection}


We tried also different feature parameters: 


normalized Frequencies, term frequency inverse document frequency (TF-IDF), binary. The binary Parameters gave better results as compared to other
parameters.


\subsection{Perceptron Settings}
We tried different repetition settings for perceptron. We trained our model for different Epochs settings:
50, 100, 150, 200 and 250. The best result was with 200 epochs across the dataset.

We adjusted bias for each epoch. Initially bias was set 0 then gradually altered the figure to 0.1, 0.5 and 1.0 for training the model.

\subsection{Result}

Best performance for the system was attained when the combination of features Ngram + ... were used. We used statistical measures of accuracy,
precision, recall and F-score for emotions in our model. We are interested in showing micro and macro F-score for each individual emotion with
various feature combinations. 
Typical features that we considered in corresponding results for all eight
emotions and their micro and macro F-score are given in table.

However poor performance was observed for some classes compared to other, which is why we conducted an error analysis study on the obtained result.

\section{Future Work}

Future work in multi label classification of emotions from twitter can be extended to all the languages. For now our work focus mainly on
English tweets as it give us flexibility in making our model understand about emotions in tweets. Training the model with significant interpretation
of emoticons can be useful in determining emotion from tweets. Due to ambiguity in use of emoticons, makes it hard for better training of model.


All the stop words including determiners, punctuation marks and functional verbs have to be removed according to criteria that maximizes the learning efficiency of model. For instance full stops, comma or functional verbs like “to be, have,
is” needs to be removed from training set as these are frequently occurring words and does not show any clue in emotion detection.


Words with longer length are often less frequent in corpus and gives very low indication in emotion detection. Words segmentation of such longer
words needs to be formulated in a better way to maximize the learning efficiency of model and ultimately better performance on test set.



-------------------------------------------------------------
\begin{table*}[t]
\small
  \centering
\begin{tabular}{|l|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}}
\hline
\begin{tabular}[c]{@{}l@{}}\end{tabular} & \bf Ngrams & \bf Ngrams+ Negation & \bf Ngram+NRC & \bf Ngram+ Negation+ NRC 
 \\ \hline
\bf Happy &  &  &  &  \\ \hline
\bf Sad &  &  &  &   \\ \hline
\bf Anger &  &  &  & \\ \hline
\bf Surprise &  &  &  &  \\ \hline
\bf Disgust &  &  &  &    \\ \hline
\bf Fear &  &  &  &  \\ \hline
\bf Love &  &  &  &  \\ \hline
\bf Trust &  &  & 1 & 1\\ \hline
\bf Macro F-score &  &  &  & \\ \hline
\end{tabular}
  \caption{  F-Score per category using Snowball Stemmer(training set) }
  \label{tab:1}
\end{table*}
-----------------

\begin{table*}[t]
\small
  \centering
\begin{tabular}{|l|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}}
\hline
\begin{tabular}[c]{@{}l@{}}\end{tabular} & \bf Ngrams & \bf Ngrams+ Negation & \bf Ngram+NRC & \bf Ngram+ Negation+ NRC 
 \\ \hline
\bf Happy &  &  &  &  \\ \hline
\bf Sad &  &  &  &   \\ \hline
\bf Anger &  &  &  & \\ \hline
\bf Surprise &  &  &  &  \\ \hline
\bf Disgust &  &  &  &    \\ \hline
\bf Fear &  &  &  &  \\ \hline
\bf Love &  &  &  &  \\ \hline
\bf Trust &  &  & 1 & 1\\ \hline
\bf Macro F-score &  &  &  & \\ \hline
\end{tabular}
  \caption{  F-Score per category using Snowball Stemmer(test set)}
  \label{tab:1}
\end{table*}
-------------------------------------------------------------


\begin{table*}[t]
\small
  \centering
\begin{tabular}{|l|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}}
\hline
\begin{tabular}[c]{@{}l@{}}\end{tabular} & \bf Ngrams & \bf Ngrams+ Negation & \bf Ngram+NRC & \bf Ngram+ Negation+ NRC 
 \\ \hline
\bf Happy &  &  &  &  \\ \hline
\bf Sad &  &  &  &   \\ \hline
\bf Anger &  &  &  & \\ \hline
\bf Surprise &  &  &  &  \\ \hline
\bf Disgust &  &  &  &    \\ \hline
\bf Fear &  &  &  &  \\ \hline
\bf Love &  &  &  &  \\ \hline
\bf Trust &  &  & 1 & 1\\ \hline
\bf Macro F-score &  &  &  & \\ \hline
\end{tabular}
  \caption{  F-Score per category using Standford Stemmer (training set) }
  \label{tab:1}
\end{table*}
=======================================================
\begin{table*}[t]
\small
  \centering
\begin{tabular}{|l|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}}
\hline
\begin{tabular}[c]{@{}l@{}}\end{tabular} & \bf Ngrams & \bf Ngrams+ Negation & \bf Ngram+NRC & \bf Ngram+ Negation+ NRC 
 \\ \hline
\bf Happy &  &  &  &  \\ \hline
\bf Sad &  &  &  &   \\ \hline
\bf Anger &  &  &  & \\ \hline
\bf Surprise &  &  &  &  \\ \hline
\bf Disgust &  &  &  &    \\ \hline
\bf Fear &  &  &  &  \\ \hline
\bf Love &  &  &  &  \\ \hline
\bf Trust &  &  & 1 & 1\\ \hline
\bf Macro F-score &  &  &  & \\ \hline
\end{tabular}
  \caption{   F-Score per category using Standford Stemmer (test set) }
  \label{tab:1}
\end{table*}
========================================================
\begin{table*}[t]
\small
  \centering
\begin{tabular}{|l|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}}
\hline
\begin{tabular}[c]{@{}l@{}}\end{tabular} & \bf Ngrams & \bf Ngrams+ Negation & \bf Ngram+NRC & \bf Ngram+ Negation+ NRC 
 \\ \hline
\bf Happy &  &  &  &  \\ \hline
\bf Sad &  &  &  &   \\ \hline
\bf Anger &  &  &  & \\ \hline
\bf Surprise &  &  &  &  \\ \hline
\bf Disgust &  &  &  &    \\ \hline
\bf Fear &  &  &  &  \\ \hline
\bf Love &  &  &  &  \\ \hline
\bf Trust &  &  & 1 & 1\\ \hline
\bf Macro F-score &  &  &  & \\ \hline
\end{tabular}
  \caption{   F-Score with Naive Bayes per category (training set) }
  \label{tab:1}
\end{table*}
------------------------------------------------------------


\begin{table*}[t]
\small
  \centering
\begin{tabular}{|l|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}|p{2 cm}}
\hline
\begin{tabular}[c]{@{}l@{}}\end{tabular} & \bf Ngrams & \bf Ngrams+ Negation & \bf Ngram+NRC & \bf Ngram+ Negation+ NRC 
 \\ \hline
\bf Happy &  &  &  &  \\ \hline
\bf Sad &  &  &  &   \\ \hline
\bf Anger &  &  &  & \\ \hline
\bf Surprise &  &  &  &  \\ \hline
\bf Disgust &  &  &  &    \\ \hline
\bf Fear &  &  &  &  \\ \hline
\bf Love &  &  &  &  \\ \hline
\bf Trust &  &  & 1 & 1\\ \hline
\bf Macro F-score &  &  &  & \\ \hline
\end{tabular}
  \caption{   F-Score with Naive Bayes per category (test set) }
  \label{tab:1}
\end{table*}
------------------------------------------

\begin{table*}[t]
\small
  \centering
\begin{tabular}{|l|p{1 cm}|p{1 cm}|p{1 cm}|p{1 cm}|p{1 cm}|p{1 cm}|p{1 cm}|p{1 cm}|p{1 cm}|p{1 cm}}
\hline
\begin{tabular}[c]{@{}l@{}}\end{tabular} & \bf Happy & \bf Sad  & \bf Angry & \bf Surprise & \bf Fear & \bf Disgust & \bf Love & \bf Trust & \begin{tabular}[c]{@{}l@{}}\bf Micro\\ \bf F-Score\end{tabular} & \begin{tabular}[c]{@{}l@{}} \bf Macro\\ \bf F-Score\end{tabular} \\ \hline
\bf 1 & 0.49 & 0.63 & 0.81 & 0.36 & 0.61 & 0.32 & 0.71 & 0.54 & 0.9 & 0.4\\ \hline
\bf 2 & 0.78 & 0.84 & 0.88 & 0.81 & 0.81 & 0.87 & 0.84 & 0.83 & 0.9 & 0.4\\ \hline
\bf 3 & 0.88 & 0.92 & 0.93 & 0.9  & 0.9 & 0.94 & 0.91 & 0.91 & 0.9 & 0.4\\ \hline
\bf 4 & 0.94 & 0.96 & 0.96 & 0.95 & 0.95 & 0.96 & 0.96 & 0.96 & 0.9 & 0.4\\ \hline
\bf 5 & 0.97 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 & 0.98 & 0.9 & 0.4  \\ \hline
\bf 6 &  &  & 1 & 1 & 1 & 1 & 1 & 1 & 0.9 & 0.4\\ \hline
\end{tabular}
  \caption{  Score per category }
  \label{tab:1}
\end{table*}







% include your own bib file like this:
%\bibliographystyle{acl}
%\bibliography{acl2014}

\begin{thebibliography}{}

\bibitem[\protect\citename{Aho and Ullman}1972]{Aho:72}
Alfred~V. Aho and Jeffrey~D. Ullman.
\newblock 1972.
\newblock {\em The Theory of Parsing, Translation and Compiling}, volume~1.
\newblock Prentice-{Hall}, Englewood Cliffs, NJ.

\bibitem[\protect\citename{{American Psychological Association}}1983]{APA:83}
{American Psychological Association}.
\newblock 1983.
\newblock {\em Publications Manual}.
\newblock American Psychological Association, Washington, DC.

\bibitem[\protect\citename{{Association for Computing Machinery}}1983]{ACM:83}
{Association for Computing Machinery}.
\newblock 1983.
\newblock {\em Computing Reviews}, 24(11):503--512.

\bibitem[\protect\citename{Chandra \bgroup et al.\egroup }1981]{Chandra:81}
Ashok~K. Chandra, Dexter~C. Kozen, and Larry~J. Stockmeyer.
\newblock 1981.
\newblock Alternation.
\newblock {\em Journal of the Association for Computing Machinery},
  28(1):114--133.

\bibitem[\protect\citename{Gusfield}1997]{Gusfield:97}
Dan Gusfield.
\newblock 1997.
\newblock {\em Algorithms on Strings, Trees and Sequences}.
\newblock Cambridge University Press, Cambridge, UK.

\end{thebibliography}

\end{document}
